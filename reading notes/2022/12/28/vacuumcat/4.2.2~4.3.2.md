### 2022.12.28
本日章节 第四章 决策树  4.2.2~4.3.2

### 第四章 决策树
4.2.2 增益率  
4.2.2提到了一种情况——”编号”作为一种属性，其信息增益远大于其他划分属性，这是因为每个分支仅有一个样本，这样的分支节点的纯度已经是最大了，但这又很显然不符合我们的需求。  
C4.5决策树算法不直接使用信息增益，而是用”增益率”来选择最优划分属性。增益率的定义如式(4.3)所示，C4.5的思想就是先找到信息增益高于平均水平的属性，再选择增益率最高的  

4.2.3 基尼系数  
基尼系数表示了数据集D的纯度，基尼系数越小，纯度越高，因此CART决策树使用划分后基尼系数最小的属性作为最优划分属性。  

#### 4.3 剪枝处理  
为了避免决策树过拟合，需要在一定的情况下对决策树进行剪枝，这其实很好理解。  
剪枝分为预剪枝和后剪枝，预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，如果结点的划分不能带来泛化性能的提升，则停止划分当前节点为叶结点——这就带来了第一个问题，什么情况下可以认为这种划分没有使得决策树的泛化性能提升呢？  
而后剪枝，很显然就是在生成完整的决策树后，对非叶结点进行考察，如果将这个子树替换为叶节点可以带来泛化性能提升，那么也可以将这个子树替换为叶节点。  
如何判断泛化性能？可以使用2.2节介绍的性能评估方法。比如留出法，使用一部分数据作为验证集来对性能进行评估。  
（但其实看到这里我又有另一个问题……总感觉这样的方法会带来极大的性能浪费，难道说剪枝一次就验证一次吗？不知道有没有什么好方法啊）  

4.3.1 预剪枝  
略，思想上面已经说过了，说实话看到这觉得决策树还真是个通俗易懂的想法……其实和人类判断的逻辑差不多，甚至有种抱着答案找解法的感觉。  

4.3.2 后剪枝  
与上面类似，略。  

P83 To be continued