##### System components

用于构建键值存储的核心组件和技术

-   Data partition
-   Data replication
-   Consistency
-   Inconsistency resolution
-   Handling failures
-   System architecture diagram
-   Write path
-   Read path

##### Data partition

数据分区的两个挑战：

-   将数据平均分配到多个服务器上
-   添加或删除节点时尽量减少数据移动

一致性散列是解决这些问题的绝佳技术

使用一致散列对数据进行分区具有以下优势

-   Automatic scaling
    -   服务器可根据负载情况自动添加和移除
-   Heterogeneity
    -   服务器的虚拟节点数量与服务器容量成正比。例如，容量越大的服务器分配的虚拟节点越多



##### Data replication

为了实现高可用性和可靠性，必须在 N 台服务器上异步复制数据，其中 N 是一个可配置的参数。这 N 台服务器的选择逻辑如下：在密钥映射到哈希环上的某个位置后，从该位置顺时针方向走，选择哈希环上的前 N 台服务器来存储数据副本。

![](https://inasa.dev/image/systemdesign/06/6.png)

key0 复制到 s1、s2 和 s3

对于虚拟节点，环上的前 N 个节点可能由少于 N 台物理服务器所拥有。为避免这一问题，我们在执行顺时针方向行走逻辑时，只选择唯一的服务器。

由于停电、网络问题、自然灾害等原因，同一数据中心的节点经常会同时出现故障。为了提高可靠性，需要将副本放置在不同的数据中心，并通过高速网络将数据中心连接起来。

##### Consistency

由于数据是在多个节点上复制的，因此必须在各个副本之间同步。Quorum consensus（法定人数共识）可以保证读写操作的一致性。

-   N =  副本数量
-   W = 写操作必须得到 W 个副本的确认，才算成功
-   R = 读操作必须等待至少 R 个副本的响应，读操作才算成功

![](https://inasa.dev/image/systemdesign/06/7.png)

N = 3

W = 1 并不意味着数据写入一台服务器, 数据在 s0、s1 和 s2 上复制

W = 1 意味着协调器必须至少收到一次确认，才能认为写操作成功

如果从 s1 收到确认，就不再需要等待来自 s0 和 s2 的确认

协调器是客户端和节点之间的代理



W、R 和 N 的配置是延迟和一致性之间的典型权衡

如果 W = 1 或 R = 1，操作会很快返回，因为协调器只需等待任何一个副本的响应

如果 W 或 R > 1，系统会提供更好的一致性；但查询速度会变慢，因为协调器必须等待速度最慢的副本的响应

如果 W + R > N，则保证了强一致性，因为必须至少有一个重叠节点拥有最新数据，以确保一致性



如何配置 N、W 和 R 以适应使用情况？

-   R = 1，W = N，系统优化为快速读取
-   W = 1，R = N，系统优化为快速写入
-   W + R > N，保证了强一致性（通常 N = 3，W = R = 2）
-   W + R <= N，就不能保证强一致性



###### Consistency models

一致性模型是设计键值存储时需要考虑的另一个重要因素。一致性模型定义了数据的一致性程度，目前存在多种可能的一致性模型：

-   Strong consistency
    -   任何读取操作都会返回一个与最新写入数据项结果相对应的值。客户端永远不会看到过时的数据
-   Weak consistency
    -   后续读取操作可能看不到最新的值
-   Eventual consistency
    -   特殊形式的弱一致性。只要有足够的时间，所有更新都会被传播，所有副本都是一致的



强一致性通常是通过强制副本在每个副本都同意当前写入之前不接受新的读/写操作来实现的

这种方法对于高可用系统来说并不理想，因为它可能会阻塞新操作

Dynamo 和 Cassandra 采用最终一致性，这也是推荐的键值存储一致性模型

从并发写入来看，最终一致性允许不一致的值进入系统，并迫使客户端读取这些值进行调节（利用版本控制进行调节）



##### Inconsistency resolution: versioning

复制提供了高可用性，但会造成复制之间的不一致

版本控制和矢量锁可用于解决不一致问题

版本控制意味着将每次数据修改都视为一个新的不可变数据版本。



不一致如何发生的

![](https://inasa.dev/image/systemdesign/06/8.png)

复制节点 n1 和 n2 的值相同

服务器 1 和服务器 2 在执行 get("name") 操作时获得相同的值

![](https://inasa.dev/image/systemdesign/06/9.png)

服务器 1 将名称改为 "johnSanFrancisco"，服务器 2 将名称改为 "johnNewYork"，两项更改同时进行

现在，有了相互冲突的值，即版本 v1 和版本 v2

为了解决这个问题，需要一个能够检测冲突并调和冲突的版本系统

vector clock（矢量时钟）是解决这一问题的常用技术

矢量时钟是与数据项相关联的 $[server, version]$ 对，用于检查一个版本是否在其他版本之前、成功或冲突。

假设一个矢量时钟$D([S1, v1], [S2, v2], …, [Sn, vn])$，D表示数据项，v1是版本计数器，s1是服务器编号

如果数据项 $D$ 被写入服务器 $Si$，系统必须执行以下任务之一

-   递增$vi$，如果$[Si, vi]$

-   否则，创建一个项$ [Si, 1]$

![](https://inasa.dev/image/systemdesign/06/10.png)



使用矢量时钟，如果 Y 的矢量时钟中每个参与者的版本计数器大于或等于版本 X 中的计数器，则很容易判断版本 X 是版本 Y 的祖先（即不冲突）。

如果 Y 的矢量时钟中有任何参与者的计数器小于 X 的相应计数器，则可以判断版本 X 是 Y 的同胞（即存在冲突）：$ D([s0, 1], [s1、 2]）$和 $D（[s0, 2]，[s1, 1]）$。

矢量时钟的缺点

-   增加了客户端的复杂性，需要实现冲突解决逻辑
-   矢量时钟中的 $[server: version]$ 对可能会迅速增加
    -   为长度设置了一个阈值，如果长度超过限制，最老的数据对就会被删除
    -   可能会导致调节效率低下，因为无法准确确定后代关系
    -   Dynamo的论文中说没有遇到问题，不知道



##### Handling failures

###### Failure detection

在分布式系统中，不能因为另一台服务器说某台服务器宕机了，就认为它宕机了。通常情况下，至少需要两个独立的信息源才能标记服务器宕机。

all-to-all multicasting （全对全多播）是一种直接的解决方案。然而，当系统中有许多服务器时，这种方法的效率很低。

![](https://inasa.dev/image/systemdesign/06/11.png)



更好的解决方案是使用分散式故障检测方法，如gossip protocol（流言协议）

-   每个节点都维护一个节点成员列表，其中包含成员 ID 和心跳计数器
-   每个节点定期递增其心跳计数器
-   每个节点定期向一组随机节点发送心跳，这些心跳又传播给另一组节点
-   节点收到心跳后，成员名单就会更新为最新信息
-   如果心跳没有在规定的时间增加，则成员被视为离线

![](https://inasa.dev/image/systemdesign/06/12.png)



-   s0 维护着节点成员列表
-   s0 注意到 s2（成员 ID = 2）的心跳计数器长时间没有增加
-   s0 向一组随机节点发送包含 s2 信息的心跳。一旦其他节点确认 s2 的心跳计数器很长时间没有更新，s2 就会被标记为停机，这一信息也会传播到其他节点



###### Handling temporary failures

在通过 gossip协议检测到故障后，系统需要部署某些机制来确保可用性

在严格quorum法定人数方法中，读写操作可能会被阻止

为了提高可用性，系统采用了一种名为 sloppy quorum 的技术

系统不强制执行法定人数要求，而是选择哈希环上前 W 个健康的服务器进行写操作，前 R 个健康的服务器进行读操作。离线服务器将被忽略。

如果一台服务器因网络或服务器故障而无法使用，另一台服务器将暂时处理请求

当故障服务器恢复运行时，将推回更改以实现数据一致性。这个过程被称为hinted handoff（提示移交）

![](https://inasa.dev/image/systemdesign/06/13.png)

由于 s2 不可用，读取和写入将暂时由 s3 处理。当 s2 重新上线后，s3 将把数据交还给 s2。



###### Handling permanent failures

Hinted handoff提示移交用于处理临时故障。

 如果副本永久不可用怎么办？为了处理这种情况，实施anti-entropy protocol反熵协议，以保持副本同步。

 反熵包括比较副本上的每条数据，并将每个副本更新为最新版本。

Merkle tree用于检测不一致性，并最大限度地减少数据传输量。

>   "哈希树或梅克尔树是一种树，其中每个非叶节点都标有其子节点的标签或值（如果是叶子）的哈希值。散列树能高效、安全地验证大型数据结构的内容"。

假设key space为 1 到 12，下面的步骤展示了如何构建梅克尔树。高亮方框表示不一致。

![](https://inasa.dev/image/systemdesign/06/14.png)

步骤 1：将key space分成若干个bucket（例子中为 4 个）。 一个桶被用作根节点，以保持树的有限深度。

![](https://inasa.dev/image/systemdesign/06/15.png)

步骤 2：创建bucket后，使用统一散列方法对bucket中的每个key进行散列。

![](https://inasa.dev/image/systemdesign/06/16.png)

步骤 3：为每个水桶创建一个散列节点

![](https://inasa.dev/image/systemdesign/06/17.png)

步骤 4：通过计算子代的哈希值，向上建立树，直到根部



要比较两棵 Merkle 树，首先要比较根哈希值。如果根哈希值匹配，则两个服务器拥有相同的数据。如果根哈希值不一致，则先比较左侧子哈希值，再比较右侧子哈希值。你可以遍历树，找到未同步的数据桶，并同步这些数据桶。并只同步这些数据桶。

使用 Merkle 树，需要同步的数据量与两个副本之间的差异成正比，而不是它们所包含的数据量。在现实世界的系统中，数据桶的大小相当大。例如，可能的配置是每 10 亿个key有 100 万个bucket，因此每个bucket只包含 1000 个key。



###### Handling data center outage

停电、网络中断、自然灾害等都可能导致数据中心中断。要建立一个能够处理数据中心中断的系统，必须在多个数据中心之间复制数据。即使一个数据中心完全断网，用户仍可通过其他数据中心访问数据。



##### System architecture diagram

![](https://inasa.dev/image/systemdesign/06/18.png)

该架构的主要特点

-   客户端通过简单的应用程序接口与键值存储进行通信：get(key) 和 put(key, value)
-   协调器是客户端和键值存储之间的代理节点
-   节点通过一致的散列分布在一个环上
-   系统完全分散，因此可以自动添加和移动节点
-   数据在多个节点复制
-   没有单点故障，因为每个节点都承担着相同的责任

![](https://inasa.dev/image/systemdesign/06/19.png)



##### Write path

![](https://inasa.dev/image/systemdesign/06/20.png)



##### Read path

![](https://inasa.dev/image/systemdesign/06/21.png)

![](https://inasa.dev/image/systemdesign/06/21.png)



Pp. 92-109