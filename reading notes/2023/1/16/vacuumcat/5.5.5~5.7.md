### 2023.1.14
本日章节 第五章

### 第五章 神经网络
#### 5.5 其他常见神经网络  
5.5.5 Elman网络 
说实话总觉得这里看起来不是很明晰，如果认真看，可能一个网络都能看很久，先囫囵吞枣式地过一遍吧。  
与前馈神经网络不同，”递归神经网络(recurrent neural networks)”允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号——这使网络在t时刻的输出状态不仅与t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化。  
这个思路可以说是非常顺理成章，但我不得不产生一个疑问——那是否有网络可以实现不仅与t-1时的网络状态有关，与t-n时的网络状态也有关呢？如果实现了那样的关联，感觉计算量和训练难度也会非常巨大，又该如何处理呢？不知道后面的章节会不会有内容来解答我的疑惑。  

5.5.6 Boltzmann机  
神经网络中有一类模型是为网络状态定义一个”能量”，能量最小化时网络达到理想状态。  
我大概明白这个的概念了，但感觉这玩意训练的方式应该会很不直观——而且它是一个全连接图，复杂度巨高。  
为了解决这个问题，现实中人们使用受限Boltzmann机进行训练。  
公式不看了，看不懂（

#### 5.6 深度学习  
哇……本节的字我都看得懂，但看不懂（）
其实也并不是看不懂，而是我无法理解这是为什么——为什么误差在多隐层内传播时会”发散”而不能收敛到稳定状态？为什么”预训练+微调”就可以视为将大量参数分组？  
为什么多个卷积层+多个采样层可以完成识别数字的任务？为什么要这么安排？多少层和顺序是否有要求？是否有坚实的理论支撑？是否还有改进的空间？如果有，该怎么改进？  
我唯一看得懂的就是后面这里可以将深度学习视为”特征学习”或者”表示学习”，但依旧是云里雾里。  
我特么问题太多了。  
我不理解。  

#### 5.7 阅读材料
略  

P121 To be continued