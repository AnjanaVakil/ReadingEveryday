### 2023.1.10
本日章节 第五章

### 第五章 神经网络
累！
#### 5.3 误差逆传播算法  
笑死，我在看感知机或者说M-P神经元的时候就在想，这玩意能不能加个反馈，果然这个想法肯定有人想过了。
通过图5.7可以看到，需要调整的参数总共有q×l+d×q+q+l个参数需要调整。  
卧槽，我发现个很了不得的东西——式5.7，其实就是偏导中的链式法则，不得不说这个法则和实际的东西联系起来，我还是第一次（可能是学高数太不认真了哈哈哈，因为这个链式法则很好记忆，都没怎么往这里想）  
最后得到了5.11，总体来说BP算法与其他算法还是很相似的，如图5.8所示。
当然，这里是针对单个误差Ek而言得到的算法，还可以将其变化为基于累积误差最小化的算法。  
如P105所言，这两种算法结合使用可能是一种比较好的思路。 
为了缓解BP神经网络的过拟合问题，可以使用”早停”或者”正则化”策略。  
早停即当训练误差降低、验证集误差升高时停止训练。  
正则化则是在误差目标函数中增加一个描述网络复杂度的部分。
说实话这两种方法应该是有效的（不然也不会被大家采用），但总觉得似乎缺少确凿的理论基础，不知道有没有相关的文献。 

P106 To be continued