- DiskANN 主要解决什么问题？  
	- the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS: high-recall, low query latency and high density(points indexed per node  
- ANN的常见解决方案  
	- k-d tree  
		- while k-d trees generate compact indices that are fast to search in low dimensions, they are typically very slow when dimension d exceeds about 20  
	- locality sensitive hashing  
		- provide near-optimal guarantees on the tradeoff between index size and search time in the worst case, but they fail to exploit the distribution of the points and are outperformed by more recent graph-based methods on real-world datasets  
	- graph-based algorithms - HNSW and NSG  
-  
- 有两种典型大数据集索引的方式:  
	- 1. Inverted Index + Data Compression  
		- 将 dataset 聚类分成 M 个不同的分区，仅仅查询 m << M 个和 query 相近的分区  
		- 由于用了压缩 相对来说使用的内存比较少 但是 1-recall@1 不是特别好 在许多场景下不可接受  
	- 2. divide the dataset into disjoint shards, and build an in-memory index for each shard  
		- 和前者的区别比较明显在于前者的分区是有空间信息的，后者则没有，需要所有的 shard 一起检索再合并  
	- 但这两种方式都需要在内存中构建索引 - 一旦落盘则性能下降严重  
		- The scalability of both these classes of algorithms is limited by the fact that they construct indices meant to be served from main memory. Moving these indices to disks, even SSDs, would result in a catastrophic rise of search latency and a corresponding drop in throughput  
-  
- 磁盘上构建索引的挑战  
	- (a) the number of random SSD accesses to a few dozen  
	- (b) the number of round trip requests to disk to under ten, preferably five  
-  
- Vamana 解决了这些问题  
	- 性能好 - DiskANN can index and serve a billion point dataset in 100s of dimensions on a workstation with 64GB RAM, providing 95%+ 1-recall@1 with latencies of under 5 milliseconds.  
	- 更小的图 - A new algorithm called Vamana which can generate graph indices with smaller diameter than NSG and HNSW, allowing DiskANN to minimize the number of sequential disk reads.  
	- 支持纯内存，性能媲美 HNSW - The graphs generated by Vamana can be also be used in-memory, where their search performance matches or exceeds state-of-the-art in-memory algorithms such as HNSW and NSG.  
	- Smaller Vamana indices for overlapping partitions of a large dataset can be easily merged into one index that provides nearly the same search performance as a single-shot index constructed for the entire dataset. This allows indexing of datasets that are otherwise too large to fit in memory.  
	- 支持现有 vector compression 方案 We show that Vamana can be combined with off-the-shelf vector compression schemes such as product quantization to build the DiskANN system. The graph index along with the full-precision vectors of the dataset are stored on the disk, while compressed vectors are cached in memory.  
-  
-  
-  
-  
- reference:  
	- TODO W. Li, Y. Zhang, Y. Sun, W. Wang, M. Li, W. Zhang, and X. Lin. Approximate nearest neighbor search on  
	  high dimensional data - experiments, analyses, and improvement. IEEE Transactions on Knowledge and  
	  Data Engineering, pages 1–1, 2019. doi: 10.1109/TKDE.2019.2909204.  
		- it's a survey  
	- TODO Sunil Arya and David M. Mount. Approximate nearest neighbor queries in fixed dimensions. In Proceed-  
	  ings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’93, pages 271–280,  
	  Philadelphia, PA, USA, 1993. Society for Industrial and Applied Mathematics. ISBN 0-89871-313-7. URL  
	  http://dl.acm.org/citation.cfm?id=313559.313768.  
		- what is sparse neighborhood graph  
	-  
	-  
	-  